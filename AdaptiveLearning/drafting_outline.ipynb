{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To figure out:\n",
    "1. Find a toy problem that this problem solves. One where it does poorly on biased data or poorly on data that accuracy falls sharply as the number of samples decreases (clinical trials?S)\n",
    "2. Find a way to theoretically describe how well it should do to overcome the above bias\n",
    "3. Find a way to generalize to categorical data\n",
    "4. Apply it to deep learning \n",
    "5. Find a more mathematical way of approximating the pdf, rather than just clustering\n",
    "6. Read similar papers to know how to format it\n",
    "7. Doping for this paper or the next?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Label by KNN and then train on the unlabeled data (relabeled)\n",
    "sample red, label it by knn and then train it on that\n",
    "\n",
    "talk about a pathological example of how different red and green data\n",
    "\n",
    "\n",
    "1. 10 hour literature search\n",
    "    SMOTE\n",
    "    Xiao Li Meng - data quality way more important than model value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Paper Outline: Optimizing Ensemble Models with Unlabeled Data for Bias Mitigation\n",
    "\n",
    "**Title**: \"Adaptive Weighting of Ensemble Classifiers Using Unlabeled Data for Bias Correction\"\n",
    "\n",
    "#### Abstract\n",
    "- Briefly introduce the problem of bias in machine learning models due to unrepresentative training data.\n",
    "- Outline the proposed solution: optimizing ensemble model weights using unlabeled data.\n",
    "- Summarize the methodology, key findings, and implications.\n",
    "\n",
    "#### Introduction\n",
    "- **Background and Motivation**:\n",
    "  - Discuss the challenges of biased training data in machine learning.\n",
    "  - Emphasize the importance of ensemble models in improving prediction accuracy.\n",
    "- **Problem Statement**:\n",
    "  - Define the specific problem addressed by the paper (bias in ensemble models).\n",
    "- **Objectives**:\n",
    "  - Detail the goals of the research (optimizing ensemble weights using unlabeled data).\n",
    "- **Contributions**:\n",
    "  - Highlight the novel contributions of the paper.\n",
    "\n",
    "#### Literature Review\n",
    "- **Ensemble Learning**:\n",
    "  - Review existing literature on ensemble methods and their strengths.\n",
    "- **Bias in Machine Learning**:\n",
    "  - Discuss research on bias in training data and its impact.\n",
    "- **Unsupervised and Semi-Supervised Learning**:\n",
    "  - Summarize key developments in using unlabeled data for model training.\n",
    "- **Weight Optimization in Ensembles**:\n",
    "  - Review existing methods and identify gaps your research addresses.\n",
    "\n",
    "#### Methodology\n",
    "- **Model Framework**:\n",
    "  - Describe the ensemble model structure and component classifiers.\n",
    "- **Data Description**:\n",
    "  - Detail the characteristics of both the original and new unlabeled data sets.\n",
    "- **Optimization Process**:\n",
    "  - Explain the mathematical formulation of the weight optimization.\n",
    "  - Discuss the choice of optimization algorithms and their justification.\n",
    "- **Regularization and Constraints**:\n",
    "  - Detail the regularization techniques used and their purpose.\n",
    "\n",
    "#### Experimentation and Results\n",
    "- **Experimental Setup**:\n",
    "  - Describe the experimental design, including data splits and baseline models for comparison.\n",
    "- **Implementation Details**:\n",
    "  - Provide specifics on the implementation, including software and hardware used.\n",
    "- **Results**:\n",
    "  - Present the results of the optimization process.\n",
    "  - Compare performance with baseline models.\n",
    "- **Discussion**:\n",
    "  - Analyze and interpret the results.\n",
    "  - Discuss the effectiveness of the method in reducing bias.\n",
    "\n",
    "#### Validation and Robustness Checks\n",
    "- **Cross-Validation**:\n",
    "  - Detail the cross-validation process used on the unlabeled data.\n",
    "- **Sensitivity Analysis**:\n",
    "  - Discuss the robustness of the model to changes in data and parameters.\n",
    "\n",
    "#### Implications and Applications\n",
    "- **Practical Implications**:\n",
    "  - Discuss how this method can be applied in real-world scenarios.\n",
    "- **Theoretical Contributions**:\n",
    "  - Highlight how the findings contribute to the existing body of knowledge.\n",
    "\n",
    "#### Limitations and Future Work\n",
    "- **Limitations**:\n",
    "  - Acknowledge any limitations or potential biases in your research.\n",
    "- **Future Research Directions**:\n",
    "  - Suggest areas for further exploration and improvement.\n",
    "\n",
    "#### Conclusion\n",
    "- Summarize the key findings and their significance.\n",
    "- Reinforce the importance of the research in the context of bias mitigation in machine learning.\n",
    "\n",
    "#### References\n",
    "- Cite all sources used in the paper following the chosen formatting style.\n",
    "\n",
    "#### Appendices (if necessary)\n",
    "- Include additional data, mathematical derivations, or experimental details.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Problem Formulation\n",
    "\n",
    "The objective is to minimize the negative log-likelihood of the observed data under a weighted combination of models:\n",
    "\n",
    "$$\n",
    "\\text{minimize}_{\\mathbf{w}} \\quad -\\sum_{i=1}^{n} \\ln(\\mathbf{w}^T P(\\mathbf{x}_i))\n",
    "$$\n",
    "\n",
    "Subject to the constraint that the weights sum up to 1:\n",
    "\n",
    "$$\n",
    "\\text{subject to} \\quad \\sum_{k=1}^{K} w_k = 1, w_k>0\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{w} = [w_1, w_2, \\ldots, w_K]^T $ are the weights of the models.\n",
    "- $P(\\mathbf{x}_i) = [p_1(\\mathbf{x}_i | m_1), p_2(\\mathbf{x}_i | m_2), \\ldots, p_K(\\mathbf{x}_i | m_K)]^T $ is the vector of probabilities for each model $ m_k $ given the observation $ \\mathbf{x}_i $.\n",
    "\n",
    "### Probability Distribution for Each Model\n",
    "\n",
    "Each model $ m_k $ provides a probability distribution based on the condition of the observation:\n",
    "\n",
    "$$\n",
    "p_k(\\mathbf{x}_i | m_k) = \n",
    "\\begin{cases} \n",
    "p_{k1} & \\text{if } \\mathbf{x}_i \\in l_{k1} \\\\\n",
    "\\vdots \\\\\n",
    "p_{kh} & \\text{if } \\mathbf{x}_i \\in l_{kh}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $p_{kj}$ are probabilities corresponding to different conditions $l_{kj}$ for model $m_k$.\n",
    "- $ \\sum_{j=1}^{h} p_{kj} = 1 $ for each model $ m_k $, ensuring a valid probability distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proving objective function is convex\n",
    "$$\n",
    "-\\sum_{i=1}^{n} \\ln(\\mathbf{w}^T P(\\mathbf{x}_i))\n",
    "$$\n",
    "\n",
    "\n",
    "1. **Convexity of the Logarithm Function**:\n",
    "   - The natural logarithm function, $\\ln(x)$, is concave for $x > 0$. This is a well-known property, and its proof typically involves showing that the second derivative of $\\ln(x)$ is negative ($\\frac{d^2}{dx^2}\\ln(x) = -\\frac{1}{x^2} < 0$ for $x > 0$).\n",
    "\n",
    "2. **Negative of a Concave Function is Convex**:\n",
    "   - The negative of a concave function is convex. Since \\(\\ln(x)\\) is concave, \\(-\\ln(x)\\) is convex.\n",
    "\n",
    "3. **Linear Combinations Maintain Convexity**:\n",
    "   - The function \\(\\mathbf{w}^T P(\\mathbf{x}_i)\\) is a linear combination of the elements in \\(P(\\mathbf{x}_i)\\) with weights \\(\\mathbf{w}\\). Linear combinations preserve convexity. Therefore, if \\(P(\\mathbf{x}_i)\\) is convex (or linear, as in this case), then so is \\(\\mathbf{w}^T P(\\mathbf{x}_i)\\).\n",
    "\n",
    "4. **Composition Rule for Convex Functions**:\n",
    "   - The composition of a convex function with an affine (linear) function is convex, provided that the convex function is non-decreasing. In our case, the convex function is \\(-\\ln(x)\\), and it is composed with the affine function \\(\\mathbf{w}^T P(\\mathbf{x}_i)\\). However, we need to be cautious here: \\(-\\ln(x)\\) is decreasing, not non-decreasing. Hence, this rule does not directly apply.\n",
    "\n",
    "5. **Addressing the Composition**:\n",
    "   - Given that \\(-\\ln(x)\\) is decreasing, we can't directly use the standard composition rule for convex functions. However, we know that \\(\\mathbf{w}^T P(\\mathbf{x}_i)\\) is positive (since it represents a combination of probabilities), and \\(-\\ln(\\mathbf{w}^T P(\\mathbf{x}_i))\\) remains convex as long as \\(\\mathbf{w}^T P(\\mathbf{x}_i) > 0\\).\n",
    "\n",
    "6. **Sum of Convex Functions**:\n",
    "   - The sum of convex functions is convex. Therefore, summing \\(-\\ln(\\mathbf{w}^T P(\\mathbf{x}_i))\\) over \\(i\\) preserves convexity.\n",
    "\n",
    "In summary, your objective function \\(-\\sum_{i=1}^{n} \\ln(\\mathbf{w}^T P(\\mathbf{x}_i))\\) is convex, given that each \\(\\mathbf{w}^T P(\\mathbf{x}_i)\\) is positive, and the negative logarithm function is convex over the positive domain. The proof hinges on the convexity of the negative logarithm and the fact that linear combinations and sums of convex functions retain convexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Papers:\n",
    "1. Original introduction of idea\n",
    "2. Doping/Sample Selection (using the ideas of priors and a bayesian approach to selecting the data to dope your model with)\n",
    "3. Applicaton of idea to Neural Nets\n",
    "4. (possible) Potential complexity speedups of the process\n",
    "5. (possible) Bayesian alernative to updating the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\text{Let } & f_x \\text{ and } f_y \\text{ be the prior distributions of } X \\text{ and } Y \\text{ respectively.} \\\\\n",
    "& \\tilde{f}_x \\text{ and } \\tilde{f}_y \\text{ denote the posterior distributions of } X \\text{ and } Y. \\\\\n",
    "& K_x \\text{ is a normalization constant.} \\\\\n",
    "\\text{Given:} & \\, \\tilde{f}_y \\text{ is known.} \\\\\n",
    "\\text{The posterior of } X \\text{ is given by:} \\\\\n",
    "\\tilde{f}_x & = \\frac{P(D|X,\\tilde{Y})P(X|\\tilde{Y})}{K_x} \\\\\n",
    "& = \\frac{\\prod_{i=1}^{n} P(z_i|X,\\tilde{Y}) P(X|\\tilde{Y})}{K_x} \\\\\n",
    "\\text{Assuming } X + \\tilde{Y} & \\text{ is distributed according to the convolution } f_x * \\tilde{f}_y, \\\\\n",
    "\\tilde{f}_x & = \\frac{\\prod_{i=1}^{n} (f_{x+\\tilde{y}}(z_i))f_x(x)}{K_x} \\\\\n",
    "\\text{Similarly, for } Y\\text{, we have:} \\\\\n",
    "\\tilde{f}_y & = \\frac{\\prod_{i=1}^{n} (f_{\\tilde{x}+y}(z_i))f_y(y)}{K_y} \\\\\n",
    "\\end{align*}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
