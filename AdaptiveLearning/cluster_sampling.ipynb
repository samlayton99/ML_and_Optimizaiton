{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change the ranomd seed or n\n",
    "np.random.seed(0)\n",
    "n = 250\n",
    "smart_factor = np.random.normal(0, 15, n)\n",
    "smart_factor_norm = smart_factor / np.max(smart_factor)\n",
    "iq = 100 + smart_factor\n",
    "iq = iq.astype(int)\n",
    "education = np.round(14 + np.random.randint(-4, 2, n) + smart_factor_norm * 5).astype(int)\n",
    "female = np.random.randint(0, 2, n).astype(bool)\n",
    "age = 30 + np.random.randint(-10, 20, n)\n",
    "income_level = np.random.choice(['low', 'medium', 'high'], n, p=[0.4, 0.4, 0.2])\n",
    "criminal_val = -.15 *iq + -2.5*education + -5*female + -15*(income_level == 'high') + -10*(income_level == 'medium') + -10 * (age > 28) + np.random.randint(-20, 20, n)\n",
    "\n",
    "# take the bottom 10% of the criminal values and label them as criminals\n",
    "criminals = criminal_val > np.percentile(criminal_val, 90)\n",
    "\n",
    "# construct a pandas dataframe with the data\n",
    "df = pd.DataFrame({'iq': iq, 'education': education, 'female': female, 'age': age, 'income_level': income_level, 'criminals': criminals})\n",
    "df.loc[3, 'criminals'] = True\n",
    "df.loc[246, 'criminals'] = True\n",
    "crime_data = df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a function that returns the number of rows in a given array\n",
    "def num_rows(array):\n",
    "    \"\"\" Returns the number of rows in a given array \"\"\"\n",
    "    if array is None:\n",
    "        return 0\n",
    "    elif len(array.shape) == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return array.shape[0]\n",
    "\n",
    "# Define a function that gives the class counts of data\n",
    "def class_counts(data):\n",
    "    if len(data.shape) == 1:\n",
    "        return {data[-1]: 1}\n",
    "    labels = data[:, -1]\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    return dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question:\n",
    "    \"\"\"Questions to use in construction and display of Decision Trees.\n",
    "    Attributes:\n",
    "        column (int): which column of the data this question asks\n",
    "        value (int/float): value the question asks about\n",
    "        numeric (bool): whether the question is numeric or not\n",
    "    Methods:\n",
    "        match: returns boolean of if a given sample answered T/F\"\"\"\n",
    "\n",
    "    def __init__(self, column, value, numeric):\n",
    "        # Store attributes\n",
    "        self.column = column\n",
    "        self.value = value\n",
    "        self.is_numeric = numeric\n",
    "        \n",
    "    def split(self, data):\n",
    "        \"\"\"Splits the data into two sets based on the question\n",
    "        Parameters:\n",
    "            data ((n,m), ndarray): Data to split\n",
    "        Returns:\n",
    "            (ndarray, ndarray): Two arrays split by the question\"\"\"\n",
    "        # Split the data if it is numeric\n",
    "        if self.is_numeric:\n",
    "            bools = data[:, self.column] >= self.value\n",
    "            return data[bools], data[~bools]\n",
    "        # Handle categorical data\n",
    "        else:\n",
    "            bools = data[:, self.column] == self.value\n",
    "            return data[bools], data[~bools]\n",
    "        \n",
    "    def match(self, sample):\n",
    "        \"\"\"Returns T/F depending on how the sample answers the question\n",
    "        Parameters:\n",
    "            sample ((n,), ndarray): New sample to classify\n",
    "        Returns:\n",
    "            (bool): How the sample compares to the question\"\"\"\n",
    "        # Split the data if it is numeric\n",
    "        if self.is_numeric:\n",
    "            bools = sample[:, self.column] >= self.value\n",
    "        # Handle categorical data\n",
    "        else:\n",
    "            bools = sample[:, self.column] == self.value\n",
    "        return bools\n",
    "            \n",
    "class Leaf:\n",
    "    \"\"\"Tree leaf node\n",
    "    Attribute:\n",
    "        prediction (dict): Dictionary of labels at the leaf\"\"\"\n",
    "    def __init__(self, data):\n",
    "        # Store attributes\n",
    "        self.prediction = class_counts(data)\n",
    "\n",
    "class Decision_Node:\n",
    "    \"\"\"Tree node with a question\n",
    "    Attributes:\n",
    "        question (Question): Question associated with node\n",
    "        left (Decision_Node or Leaf): child branch\n",
    "        right (Decision_Node or Leaf): child branch\"\"\"\n",
    "    def __init__(self, question, left_branch, right_branch):\n",
    "        # Store attributes\n",
    "        self.question = question\n",
    "        self.left = left_branch\n",
    "        self.right = right_branch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that calculates the Gini impurity of a given array\n",
    "def info_gain(data, left, right, tree_type):\n",
    "    \"\"\"Return the info gain of a partition of data.\n",
    "    Parameters:\n",
    "        data (ndarray): the unsplit data\n",
    "        left (ndarray): left split of data\n",
    "        right (ndarray): right split of data\n",
    "    Returns:\n",
    "        (float): info gain of the data\"\"\"\n",
    "    if tree_type != 'classification':\n",
    "        raise NotImplementedError('Info gain only implemented for classification trees')\n",
    "\n",
    "    def gini(data):\n",
    "        \"\"\"Return the Gini impurity of given array of data.\n",
    "        Parameters:\n",
    "            data (ndarray): data to examine\n",
    "        Returns:\n",
    "            (float): Gini impurity of the data\"\"\"\n",
    "        counts = class_counts(data)\n",
    "        N = num_rows(data)\n",
    "        impurity = 1\n",
    "        for lbl in counts:\n",
    "            prob_of_lbl = counts[lbl] / N\n",
    "            impurity -= prob_of_lbl**2\n",
    "        return impurity\n",
    "        \n",
    "    p = num_rows(right)/(num_rows(left)+num_rows(right))\n",
    "    return gini(data) - p*gini(right)-(1-p)*gini(left)\n",
    "\n",
    "# Define a function that finds the best split for a given data set\n",
    "def find_best_split(data, tree_type, size_rand_subset, min_samples_leaf, is_numeric):\n",
    "    # Get feature_choice, define refinement, and initialize the best gain and current_bool\n",
    "    feature_choice = np.random.choice(data.shape[1]-1, size_rand_subset, replace=False)\n",
    "    refinement = 100/np.min([1+np.abs(data.shape[0] -2*min_samples_leaf -2)// min_samples_leaf,40])\n",
    "    current_q = None\n",
    "    current_gain = 0\n",
    "\n",
    "    # Initialize the best gain and question and loop through the first n-1 columns\n",
    "    for i in feature_choice:\n",
    "        screen = data[:,i]\n",
    "\n",
    "        # if it is numeric, get the unique split values based on the refinement\n",
    "        if is_numeric[i]:\n",
    "            split_vals = np.unique(np.percentile(screen, np.arange(refinement, 100, refinement)))\n",
    "            # Loop through the split values and partition the data\n",
    "            for value in split_vals:\n",
    "                bool_splits = (screen >= value) \n",
    "\n",
    "                # If the partition is too small, skip it\n",
    "                if (np.sum(bool_splits) < min_samples_leaf) or (np.sum(~bool_splits) < min_samples_leaf):\n",
    "                    continue\n",
    "\n",
    "                # Calculate the info gain and update the best gain and question if necessary\n",
    "                gain = info_gain(data, data[bool_splits], data[~bool_splits], tree_type)\n",
    "                if gain > current_gain:\n",
    "                    current_gain, current_q = gain, (i, value, True)\n",
    "        \n",
    "        # If it is not numeric, get the unique values and the size\n",
    "        else:\n",
    "            categories = np.unique(screen)\n",
    "            cat_size = len(categories)\n",
    "\n",
    "            # If there are only two categories, there is only one possible split\n",
    "            if cat_size == 2:\n",
    "                cat_splits = [[0]]\n",
    "\n",
    "            # If there are more than two categories, there are many different possible splits. Split individually and then choose random splits\n",
    "            else:\n",
    "                cat_splits = [i for i in range(cat_size)]\n",
    "                random_splits = [np.random.choice(categories, size, replace=False) for size in np.arange(2, cat_size-1, 1)]\n",
    "                if len(random_splits) > 0:\n",
    "                    cat_splits.append(random_splits)\n",
    "                \n",
    "            # Get the unique splits\n",
    "            for split in cat_splits:\n",
    "                bool_splits = np.isin(screen, split)\n",
    "\n",
    "                # If the partition is too small, skip it\n",
    "                if (np.sum(bool_splits) < min_samples_leaf) or (np.sum(~bool_splits) < min_samples_leaf):\n",
    "                    continue\n",
    "            \n",
    "                # Calculate the info gain and update the best gain and question if necessary\n",
    "                gain = info_gain(data, data[bool_splits], data[~bool_splits], tree_type)\n",
    "                if gain > current_gain:\n",
    "                    current_gain, current_q = gain, (i, split, False)\n",
    "    \n",
    "    # If there is no best question, return None, otherwise return the best question\n",
    "    if current_q is None:\n",
    "        return 0, None\n",
    "    else:\n",
    "        return current_gain, Question(current_q[0], current_q[1], current_q[2])\n",
    "\n",
    "# Define a function that predicts a decision tree\n",
    "def predict_tree(sample, my_tree, predictions):\n",
    "    \"\"\"Predict the label for a sample given a pre-made decision tree\n",
    "    Parameters:\n",
    "        sample (ndarray): a single sample\n",
    "        my_tree (Decision_Node or Leaf): a decision tree\n",
    "    Returns:\n",
    "        Label to be assigned to new sample\"\"\"\n",
    "\n",
    "    # Base case if my_tree is a leaf\n",
    "    if isinstance(my_tree, Leaf):\n",
    "        # Gets the most common label of the leaf\n",
    "        predictions[:] = max(my_tree.prediction, key=my_tree.prediction.get)\n",
    "        return predictions\n",
    "\n",
    "    # Otherwise break it down into left and right branches\n",
    "    bools = my_tree.question.match(sample)\n",
    "    left, right = sample[bools], sample[~bools]\n",
    "\n",
    "    # Recursively call predict_tree on the left and right branches to fill in the predictions array\n",
    "    predictions[bools] = predict_tree(left, my_tree.left, predictions[bools])\n",
    "    predictions[~bools] = predict_tree(right, my_tree.right, predictions[~bools])\n",
    "    return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Classes (DecisionTree, AdaptiveForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \"\"\"\n",
    "    A decision tree class that can be used for classification.\n",
    "    Attributes:\n",
    "        data (numpy array): data to use for the tree\n",
    "        features (list): list of feature names\n",
    "        min_samples_leaf (int): minimum number of samples in a leaf\n",
    "        max_depth (int): maximum depth of the tree\n",
    "        is_numeric (numpy array - boolean): boolean array indicating whether each feature is numeric or not\n",
    "        size_random_subset (int): number of features to use for each split\n",
    "        y (numpy array): target variable\n",
    "        X (numpy array): data without the target variable\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, tree_type, features, min_samples_leaf, max_depth, is_numeric, size_random_subset):\n",
    "        \"\"\"\n",
    "        Initialize the decision tree\n",
    "        Inputs:\n",
    "            data (numpy array): data to use for the tree (target in last column)\n",
    "            tree_type (str): type of tree (classification or regression)\n",
    "            features (list): list of feature names\n",
    "            min_samples_leaf (int): minimum number of samples in a leaf\n",
    "            max_depth (int): maximum depth of the tree\n",
    "            is_numeric (numpy array - boolean): boolean array indicating whether each feature is numeric or not\n",
    "            size_random_subset (int): number of features to use for each split\n",
    "        \"\"\"\n",
    "        # Initialize the different attributes of the tree\n",
    "        self.data = data\n",
    "        self.tree_type = tree_type\n",
    "        self.features = features\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_depth = max_depth\n",
    "        self.is_numeric = is_numeric\n",
    "        self.size_random_subset = size_random_subset\n",
    "\n",
    "    # Define a method to build a tree\n",
    "    def build_tree(self, data, current_depth=0):\n",
    "        \"\"\"Build a classification tree using the classes Decision_Node and Leaf\n",
    "        Parameters:\n",
    "            data (ndarray)\n",
    "            min_samples_leaf (int): minimum allowed number of samples per leaf\n",
    "            max_depth (int): maximum allowed depth\n",
    "            current_depth (int): depth counter\n",
    "            random_subset (bool): whether or not to train on a random subset of features\n",
    "        Returns:\n",
    "            Decision_Node (or Leaf)\"\"\"\n",
    "        # If the number of rows is less than the minimum samples per leaf, return a leaf\n",
    "        if data.shape[0] < 2*self.min_samples_leaf:\n",
    "            return Leaf(data)\n",
    "        \n",
    "        # Find the best question to ask and return a leaf if there is no gain or past max depth\n",
    "        gain, question = find_best_split(data, self.tree_type, self.size_random_subset, self.min_samples_leaf, self.is_numeric)\n",
    "        if gain == 0 or current_depth >= self.max_depth:\n",
    "            return Leaf(data)\n",
    "        \n",
    "        # Partition the data and build the left and right branches\n",
    "        left, right = question.split(data)\n",
    "        left_branch = self.build_tree(left, current_depth+1)\n",
    "        right_branch = self.build_tree(right, current_depth+1)\n",
    "\n",
    "        # Return a Decision_Node with the best question and branches\n",
    "        return Decision_Node(question, left_branch, right_branch)\n",
    "    \n",
    "    # Define a method to fit the tree\n",
    "    def fit(self):\n",
    "        self.tree = self.build_tree(self.data)\n",
    "\n",
    "    # Define a method to predict the labels for a sample\n",
    "    def predict(self, sample):\n",
    "        \"\"\"Predict the label for a sample\n",
    "        Parameters:\n",
    "            sample (ndarray): a single sample\n",
    "        Returns:\n",
    "            Label to be assigned to new sample\"\"\"\n",
    "        # Make sure the sample is a 2D array\n",
    "        if len(sample.shape) == 1:\n",
    "            sample = sample.reshape(1, -1)\n",
    "\n",
    "        # Initialize the predictions array and predict the label\n",
    "        predictions = np.zeros(len(sample))\n",
    "        return predict_tree(sample, self.tree, predictions)\n",
    "        \n",
    "class AdaptiveForest:\n",
    "    \"\"\" A random forest class that can be used for classification or regression.\n",
    "    Attributes:\n",
    "        data (numpy array): data to use for the forest\n",
    "        features (list): list of feature names\n",
    "        min_samples_leaf (int): minimum number of samples in a leaf\n",
    "        max_depth (int): maximum depth of the tree\n",
    "        is_numeric (numpy array - boolean): boolean array indicating whether each feature is numeric or not\n",
    "        category_codes (dict): dictionary of the category codes for each non numeric feature\n",
    "        size_random_subset (int): number of features to use for each split\n",
    "        n_trees (int): number of trees in the forest\n",
    "        weights (numpy array): weights for each tree\n",
    "        trees (list): list of trees in the forest\n",
    "        bootstrap_size (int): size of the boot strap sample\n",
    "        data_size (int): size of the data\n",
    "        class_n (int): number of classes\n",
    "        forest_type (str): type of forest (classification or regression)\n",
    "\n",
    "    Methods:\n",
    "        access_tree: access the decision tree class\n",
    "        print_codes: print the category codes\n",
    "        fit: fit the random forest\n",
    "        fit_with_oob: fit the random forest with out of bag data\n",
    "        predict: predict the label for a sample with the ensemble\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,dataframe, labels = None, target_index = None, n_trees = 10, min_samples_leaf=5, max_depth=4, features = None, size_rand_subset = None, bootstrap_size = None):\n",
    "        \"\"\"\n",
    "        Initialize the random forest\n",
    "        Inputs:\n",
    "            dataframe (pandas dataframe or numpy array): data to use for the forest (target in last column)\n",
    "            labels (numpy array): labels to use for the forest\n",
    "            target_index (int): index of the target variable\n",
    "            n_trees (int): number of trees in the forest\n",
    "            min_samples_leaf (int): minimum number of samples in a leaf\n",
    "            max_depth (int): maximum depth of the tree\n",
    "            features (list): list of feature names\n",
    "            size_rand_subset (int): number of features to use for each split\n",
    "            bootstrap_size (int): size of the boot strap sample\n",
    "        \"\"\"\n",
    "        #### Data preprocessing ####\n",
    "        # Check the data type and raise an error if it is not a dataframe or numpy array\n",
    "        if isinstance(dataframe, np.ndarray):\n",
    "            df = pd.DataFrame(dataframe)\n",
    "\n",
    "            # Check if the data is numeric and convert it if it is not\n",
    "            def infer_dtype(col):\n",
    "                try:\n",
    "                    return pd.to_numeric(col, errors='raise')\n",
    "                except:\n",
    "                    return col\n",
    "            df = df.apply(lambda col: infer_dtype(col))\n",
    "            if features is not None:\n",
    "                df.columns = features\n",
    "        \n",
    "        # If the data is a dataframe, then make a copy of it, raise an error otherwise\n",
    "        elif isinstance(dataframe, pd.DataFrame):\n",
    "            df = dataframe.copy(deep=True)\n",
    "        else:\n",
    "            raise ValueError('Data must be a pandas dataframe or numpy array')\n",
    "\n",
    "        # If labels are given, then make them a series and concatenate them to the dataframe if conditions met\n",
    "        if labels is not None:\n",
    "            labels = pd.Series(labels)\n",
    "            if target_index is not None:\n",
    "                raise ValueError('Cannot have both labels and target index')\n",
    "            \n",
    "            # check if labels and dataframe have the same number of rows\n",
    "            if len(labels) != len(df):\n",
    "                raise ValueError('Labels and dataframe must be same size')\n",
    "            \n",
    "            # concatenate the labels to the dataframe\n",
    "            labels.name = 'target'\n",
    "            df = pd.concat([df, labels], axis=1)\n",
    "\n",
    "        # If labels are not given, then check if target index is given and move the target variable to the last column\n",
    "        else:\n",
    "            if target_index is None:\n",
    "                print('Warning: No target index given. Using last index as target variable.')\n",
    "            else:\n",
    "                cols = list(df.columns)\n",
    "                cols.append(cols.pop(target_index))\n",
    "                df = df[cols]\n",
    "\n",
    "        #### Initialize tree attributes ####\n",
    "        # Save the features except for the target variable and check the target column data type\n",
    "        self.features = np.array(df.columns)\n",
    "        target_dtype = df[self.features[-1]].dtype\n",
    "\n",
    "        # check if the target variable is numeric\n",
    "        if isinstance(target_dtype, (int, float, complex)):\n",
    "            self.forest_type = 'regression'\n",
    "        else:\n",
    "            self.forest_type = 'classification'\n",
    "\n",
    "        # Check which features are numeric and save the boolean mask\n",
    "        numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "        self.is_numeric = df.columns.isin(numeric_cols)\n",
    "        self.category_codes = {}\n",
    "     \n",
    "        # Loop through non numeric columns and convert them to numeric while saving the category codes\n",
    "        for col in self.features[~self.is_numeric]:\n",
    "            self.category_codes[col] = dict(enumerate(df[col].astype('category').cat.categories))\n",
    "            df[col] = df[col].astype('category').cat.codes\n",
    "    \n",
    "        # Save the size of the random subset for attribute bagging\n",
    "        if size_rand_subset is None:\n",
    "            self.size_rand_subset = int(np.ceil(np.sqrt(len(self.features) - 1)))\n",
    "        else:\n",
    "            self.size_rand_subset = size_rand_subset\n",
    "\n",
    "        # Initialize data and min_samples_leaf\n",
    "        self.data = df.values.astype(float)\n",
    "        if min_samples_leaf is None:\n",
    "            self.min_samples_leaf = 1\n",
    "        else:\n",
    "            self.min_samples_leaf = min_samples_leaf\n",
    "\n",
    "        # Initialize the max depth\n",
    "        if max_depth is None:\n",
    "            self.max_depth = np.inf\n",
    "        else:\n",
    "            self.max_depth = max_depth\n",
    "\n",
    "        # Save the data size and number of classes\n",
    "        self.data_size = self.data.shape[0]\n",
    "        self.class_n = len(np.unique(self.data[:,-1]))\n",
    "\n",
    "        #### Other forest attributes ####\n",
    "        # Save the boot strap size if given\n",
    "        if bootstrap_size is None:\n",
    "            self.bootstrap_size = df.shape[0]\n",
    "        else:\n",
    "            self.bootstrap_size = bootstrap_size\n",
    "\n",
    "        # Save the number of trees, the weights, and initialize the trees\n",
    "        self.n_trees = n_trees\n",
    "        self.trees = []\n",
    "        self.data_weights = np.ones(self.data_size) / self.data_size\n",
    "\n",
    "    def access_tree(self):\n",
    "        \"\"\" Access the decision tree class \"\"\"\n",
    "        return DecisionTree(self.data, self.forest_type, self.features, self.min_samples_leaf, self.max_depth, self.is_numeric, self.size_rand_subset)\n",
    "        \n",
    "    def print_codes(self):\n",
    "        \"\"\" Print the category codes \"\"\"\n",
    "        max_key_length = max(len(str(key)) for key in self.category_codes)\n",
    "        for key in self.category_codes:\n",
    "            print(\"{:<{width}} {}\".format(str(key), \"---  \"+str(self.category_codes[key]), width=max_key_length + 1))\n",
    "\n",
    "    def fit(self, bootstrap_size = None):\n",
    "        \"\"\" Fit the random forest \"\"\"\n",
    "        self.trees = []\n",
    "        # If a new bootstrap size is given, update the bootstrap size\n",
    "        if bootstrap_size is not None:\n",
    "            self.bootstrap_size = bootstrap_size\n",
    "\n",
    "        # Loop through the number of trees and fit each tree to bootstrapped data\n",
    "        for _ in range(self.n_trees):\n",
    "            # boot strap our data, build a tree, and fit the tree\n",
    "            indices = np.random.choice(self.data_size, self.bootstrap_size, replace=True, p=self.data_weights)\n",
    "            sample = self.data[indices]\n",
    "            tree = DecisionTree(sample, self.forest_type, self.features, self.min_samples_leaf, self.max_depth, self.is_numeric, self.size_rand_subset)\n",
    "            tree.fit()\n",
    "\n",
    "            # append the tree to the forest\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, sample, labeled = False):\n",
    "        \"\"\" Predict the label for a sample with the ensemble\"\"\"\n",
    "        # Initialize the ensemble predictions and loop through the trees\n",
    "        ensemble = np.zeros((len(sample),self.n_trees))\n",
    "        for i in range(self.n_trees):\n",
    "            ensemble[:,i] = self.trees[i].predict(sample)\n",
    "\n",
    "        # Get the predicted labels\n",
    "        ensemble = ensemble.astype(int)\n",
    "        predicted = np.apply_along_axis(lambda x: np.argmax(np.bincount(x, minlength=np.max(ensemble)+1)), axis=1, arr=ensemble)\n",
    "\n",
    "        # If the labels are not given, return the predicted labels\n",
    "        if not labeled:\n",
    "            return predicted\n",
    "        \n",
    "        # Otherwise return a vector with the category codes\n",
    "        else:\n",
    "            return np.vectorize(self.category_codes[self.features[-1]].get)(predicted.astype(int))\n",
    "        \n",
    "    def cluster_probs(self, true_pop, subset, norm=2):\n",
    "        # Get the matrix of norms between the true population and the subset\n",
    "        diff = np.linalg.norm(true_pop[:,np.newaxis,:] - subset[np.newaxis,:,:], ord=norm, axis=2)\n",
    "        \n",
    "        # find the closest subset point for each true population point\n",
    "        closest = np.argmin(diff, axis=1)\n",
    "\n",
    "        # Count the number of close true population points for each subset point and return the normalized counts\n",
    "        counts = np.bincount(closest, minlength=subset.shape[0]) + 1\n",
    "        return counts / np.sum(counts)\n",
    "    \n",
    "    def update_weights(self, sample, norm = 2):\n",
    "        \"\"\" Update the weights of the trees\n",
    "        Parameters:\n",
    "            sample (ndarray): a sample of the data not including the label\n",
    "        \"\"\"\n",
    "        # Get the leaf probabilities and update the weights\n",
    "        self.data_weights = self.cluster_probs(sample, self.data[:,:-1], norm)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.971\n",
      "Time to initialize: 0.014\n",
      "Time to predict: 0.102\n",
      "Accuracy: 0.964\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# fit random forest classifier with 100 trees\n",
    "rf = RF(n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# get predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# get accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy: {:.3f}'.format(accuracy))\n",
    "\n",
    "# get confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "#display(cm)\n",
    "\n",
    "import time\n",
    "# Do the same for adaptive forest\n",
    "start = time.time()\n",
    "af = AdaptiveForest(X_train, y_train, n_trees=100, min_samples_leaf=1, max_depth=None)\n",
    "end = time.time()\n",
    "end = time.time()\n",
    "print('Time to initialize: {:.3f}'.format(end-start))\n",
    "start = time.time()\n",
    "af.fit()\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "y_pred = af.predict(X_test)\n",
    "end = time.time()\n",
    "print('Time to predict: {:.3f}'.format(end-start))\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy: {:.3f}'.format(accuracy))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "#display(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_digits(digits,biased_digits = None,sizes = None, sample_size=300):\n",
    "    \"\"\"\n",
    "    Take a biased sample of certain digits according to the sizes given\n",
    "    Parameters:\n",
    "        digits (ndarray): array of digits to sample from\n",
    "        biased_digits (ndarray): array of digits to sample more or less of\n",
    "        sizes (ndarray): array of weights for each digit\n",
    "\n",
    "    Returns:\n",
    "        (ndarray): array of indices of digits sampled\n",
    "    \"\"\"\n",
    "    # set the max bias limit\n",
    "    max_bias_limit = 1/3\n",
    "\n",
    "    # set biased_digits and sizes to arrays if they are None\n",
    "    if biased_digits is None:\n",
    "        biased_digits = np.array([])\n",
    "    else:\n",
    "        biased_digits = np.array(biased_digits)\n",
    "\n",
    "    # do the same for sizes\n",
    "    if sizes is None:\n",
    "        sizes = np.array([])\n",
    "    else:\n",
    "        sizes = np.array(sizes)\n",
    "    digits = np.array(digits)\n",
    "\n",
    "    # Check that the digits and sizes are the same length\n",
    "    if len(biased_digits) != len(sizes):\n",
    "        raise ValueError('Digits and sizes must be the same length')\n",
    "    \n",
    "    # Get the counts of each digit and initialize the digit sizes\n",
    "    counts = np.bincount(digits, minlength=10).astype(float)\n",
    "    digit_sizes = np.ones(10)\n",
    "\n",
    "    # Get digit sizes for the biased digits\n",
    "    for i in range(10):\n",
    "        if i in biased_digits:\n",
    "            digit_sizes[i] = sizes[np.where(biased_digits == i)[0][0]]\n",
    "\n",
    "    # normalize units\n",
    "    digit_sizes = digit_sizes / np.sum(digit_sizes)\n",
    "    draw_size = np.round(sample_size * digit_sizes)\n",
    "    testing = draw_size / counts\n",
    "\n",
    "    # Check that the testing is within the max bias limit\n",
    "    if np.any(testing > max_bias_limit):\n",
    "        raise ValueError('Reduce sample size given the bias weights')\n",
    "    \n",
    "    def sample_from_counts(digits, draw_size):\n",
    "        \"\"\" Sample from the darw_size \"\"\"\n",
    "        ind_list = []\n",
    "\n",
    "        # Loop through the digits and get the indices of each digit\n",
    "        for i in range(10):\n",
    "            single_digit = (digits == i)\n",
    "            digit_indices = np.where(single_digit)[0]\n",
    "            indices = np.random.choice(digit_indices, int(draw_size[i]), replace=False)\n",
    "\n",
    "            # Append the indices to the list\n",
    "            ind_list.append(indices)\n",
    "\n",
    "        # Return the indices\n",
    "        return np.concatenate(ind_list)\n",
    "    \n",
    "    # Get the indices of the biased digits and the remaining digits\n",
    "    biased_indices = sample_from_counts(digits, draw_size)\n",
    "    remaining_indices = np.setdiff1d(np.arange(len(digits)), biased_indices)\n",
    "    remaining_digits = digits[remaining_indices]\n",
    "\n",
    "    # Get the unbiased draw size\n",
    "    unbiased_ratio = counts / np.sum(counts)\n",
    "    min_dist = np.min(counts - draw_size)\n",
    "    min_dist_ind = np.argmin(counts - draw_size)\n",
    "    scale = min_dist / unbiased_ratio[min_dist_ind]\n",
    "    unbiased_draw_size = np.round(scale * unbiased_ratio) - 2\n",
    "    \n",
    "    # Get the indices of the unbiased digits and return the biased and unbiased indices\n",
    "    unbiased_indices = sample_from_counts(remaining_digits, unbiased_draw_size)\n",
    "    return biased_indices, unbiased_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy: 0.876\n",
      "AF Accuracy: 0.853\n",
      "AF Accuracy with weights: 0.879\n"
     ]
    }
   ],
   "source": [
    "bias,unbias = bias_digits(y, sample_size=100)\n",
    "x_train,x_test,y_train,y_test = X[bias],X[unbias],y[bias],y[unbias]\n",
    "\n",
    "# fit random forest classifier with 100 trees\n",
    "rf = RF(n_estimators=100)\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "# get predictions\n",
    "y_pred = rf.predict(x_test)\n",
    "\n",
    "# get accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('RF Accuracy: {:.3f}'.format(accuracy))\n",
    "\n",
    "# repeat for adaptive forest\n",
    "af = AdaptiveForest(x_train, y_train, n_trees=100, min_samples_leaf=1, max_depth=None)\n",
    "af.fit()\n",
    "y_pred = af.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('AF Accuracy: {:.3f}'.format(accuracy))\n",
    "\n",
    "# update the weights\n",
    "af.update_weights(x_test)\n",
    "af.fit()\n",
    "y_pred = af.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('AF Accuracy with weights: {:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(af.data_weights)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[167,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0, 147,   9,   6,   1,   1,   1,   2,   0,   2],\n",
       "       [  1,   1, 153,   6,   0,   0,   0,   0,   4,   0],\n",
       "       [  0,   1,   0, 159,   0,   0,   0,   4,   3,   1],\n",
       "       [  1,   1,   0,   0, 156,   1,   1,   8,   0,   1],\n",
       "       [  1,   1,   0,   0,   0, 155,   4,   0,   0,   7],\n",
       "       [  1,   4,   0,   0,   1,   1, 163,   0,   0,   0],\n",
       "       [  2,   0,   0,   0,   3,   1,   0, 159,   0,   4],\n",
       "       [  1,  12,   1,   1,   0,   2,   2,   7, 125,  10],\n",
       "       [  0,   2,   2,  17,   2,   6,   1,   1,   2, 137]])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
