{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "# import an optimize function\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change the ranomd seed or n\n",
    "np.random.seed(0)\n",
    "n = 250\n",
    "smart_factor = np.random.normal(0, 15, n)\n",
    "smart_factor_norm = smart_factor / np.max(smart_factor)\n",
    "iq = 100 + smart_factor\n",
    "iq = iq.astype(int)\n",
    "education = np.round(14 + np.random.randint(-4, 2, n) + smart_factor_norm * 5).astype(int)\n",
    "female = np.random.randint(0, 2, n).astype(bool)\n",
    "age = 30 + np.random.randint(-10, 20, n)\n",
    "income_level = np.random.choice(['low', 'medium', 'high'], n, p=[0.4, 0.4, 0.2])\n",
    "criminal_val = -.15 *iq + -2.5*education + -5*female + -15*(income_level == 'high') + -10*(income_level == 'medium') + -10 * (age > 28) + np.random.randint(-20, 20, n)\n",
    "\n",
    "# take the bottom 10% of the criminal values and label them as criminals\n",
    "criminals = criminal_val > np.percentile(criminal_val, 90)\n",
    "\n",
    "# construct a pandas dataframe with the data\n",
    "df = pd.DataFrame({'iq': iq, 'education': education, 'female': female, 'age': age, 'income_level': income_level, 'criminals': criminals})\n",
    "df.loc[3, 'criminals'] = True\n",
    "df.loc[246, 'criminals'] = True\n",
    "crime_data = df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a function that returns the number of rows in a given array\n",
    "def num_rows(array):\n",
    "    \"\"\" Returns the number of rows in a given array \"\"\"\n",
    "    if array is None:\n",
    "        return 0\n",
    "    elif len(array.shape) == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return array.shape[0]\n",
    "\n",
    "# Define a function that gives the class counts of data\n",
    "def class_counts(data):\n",
    "    if len(data.shape) == 1:\n",
    "        return {data[-1]: 1}\n",
    "    labels = data[:, -1]\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    return dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question:\n",
    "    \"\"\"Questions to use in construction and display of Decision Trees.\n",
    "    Attributes:\n",
    "        column (int): which column of the data this question asks\n",
    "        value (int/float): value the question asks about\n",
    "        numeric (bool): whether the question is numeric or not\n",
    "    Methods:\n",
    "        match: returns boolean of if a given sample answered T/F\"\"\"\n",
    "\n",
    "    def __init__(self, column, value, numeric):\n",
    "        # Store attributes\n",
    "        self.column = column\n",
    "        self.value = value\n",
    "        self.is_numeric = numeric\n",
    "        \n",
    "    def split(self, data):\n",
    "        \"\"\"Splits the data into two sets based on the question\n",
    "        Parameters:\n",
    "            data ((n,m), ndarray): Data to split\n",
    "        Returns:\n",
    "            (ndarray, ndarray): Two arrays split by the question\"\"\"\n",
    "        # Split the data if it is numeric\n",
    "        if self.is_numeric:\n",
    "            bools = data[:, self.column] >= self.value\n",
    "            return data[bools], data[~bools]\n",
    "        # Handle categorical data\n",
    "        else:\n",
    "            bools = data[:, self.column] == self.value\n",
    "            return data[bools], data[~bools]\n",
    "        \n",
    "    def match(self, sample):\n",
    "        \"\"\"Returns T/F depending on how the sample answers the question\n",
    "        Parameters:\n",
    "            sample ((n,), ndarray): New sample to classify\n",
    "        Returns:\n",
    "            (bool): How the sample compares to the question\"\"\"\n",
    "        # Split the data if it is numeric\n",
    "        if self.is_numeric:\n",
    "            bools = sample[:, self.column] >= self.value\n",
    "        # Handle categorical data\n",
    "        else:\n",
    "            bools = sample[:, self.column] == self.value\n",
    "        return bools\n",
    "            \n",
    "class Leaf:\n",
    "    \"\"\"Tree leaf node\n",
    "    Attribute:\n",
    "        prediction (dict): Dictionary of labels at the leaf\"\"\"\n",
    "    def __init__(self, data):\n",
    "        # Store attributes\n",
    "        self.prediction = class_counts(data)\n",
    "        self.train_size = num_rows(data)\n",
    "    \n",
    "    # Define a method to get the train size\n",
    "    def get_train_size(self):\n",
    "        return self.train_size\n",
    "\n",
    "    # Define a method to update the test size\n",
    "    def update_test_size(self, data):\n",
    "        self.test_size = num_rows(data)\n",
    "\n",
    "    # Define a method to get the test size\n",
    "    def get_test_size(self):\n",
    "        return self.test_size\n",
    "\n",
    "class Decision_Node:\n",
    "    \"\"\"Tree node with a question\n",
    "    Attributes:\n",
    "        question (Question): Question associated with node\n",
    "        left (Decision_Node or Leaf): child branch\n",
    "        right (Decision_Node or Leaf): child branch\"\"\"\n",
    "    def __init__(self, question, left_branch, right_branch):\n",
    "        # Store attributes\n",
    "        self.question = question\n",
    "        self.left = left_branch\n",
    "        self.right = right_branch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that calculates the Gini impurity of a given array\n",
    "def info_gain(data, left, right, tree_type):\n",
    "    \"\"\"Return the info gain of a partition of data.\n",
    "    Parameters:\n",
    "        data (ndarray): the unsplit data\n",
    "        left (ndarray): left split of data\n",
    "        right (ndarray): right split of data\n",
    "    Returns:\n",
    "        (float): info gain of the data\"\"\"\n",
    "    if tree_type != 'classification':\n",
    "        raise NotImplementedError('Info gain only implemented for classification trees')\n",
    "\n",
    "    def gini(data):\n",
    "        \"\"\"Return the Gini impurity of given array of data.\n",
    "        Parameters:\n",
    "            data (ndarray): data to examine\n",
    "        Returns:\n",
    "            (float): Gini impurity of the data\"\"\"\n",
    "        counts = class_counts(data)\n",
    "        N = num_rows(data)\n",
    "        impurity = 1\n",
    "        for lbl in counts:\n",
    "            prob_of_lbl = counts[lbl] / N\n",
    "            impurity -= prob_of_lbl**2\n",
    "        return impurity\n",
    "        \n",
    "    p = num_rows(right)/(num_rows(left)+num_rows(right))\n",
    "    return gini(data) - p*gini(right)-(1-p)*gini(left)\n",
    "\n",
    "# Define a function that finds the best split for a given data set\n",
    "def find_best_split(data, tree_type, size_rand_subset, min_samples_leaf, is_numeric):\n",
    "    # Get feature_choice, define refinement, and initialize the best gain and current_bool\n",
    "    feature_choice = np.random.choice(data.shape[1]-1, size_rand_subset, replace=False)\n",
    "    refinement = 100/np.min([1+np.abs(data.shape[0] -2*min_samples_leaf -2)// min_samples_leaf,40])\n",
    "    current_q = None\n",
    "    current_gain = 0\n",
    "\n",
    "    # Initialize the best gain and question and loop through the first n-1 columns\n",
    "    for i in feature_choice:\n",
    "        screen = data[:,i]\n",
    "\n",
    "        # if it is numeric, get the unique split values based on the refinement\n",
    "        if is_numeric[i]:\n",
    "            split_vals = np.unique(np.percentile(screen, np.arange(refinement, 100, refinement)))\n",
    "            # Loop through the split values and partition the data\n",
    "            for value in split_vals:\n",
    "                bool_splits = (screen >= value) \n",
    "\n",
    "                # If the partition is too small, skip it\n",
    "                if (np.sum(bool_splits) < min_samples_leaf) or (np.sum(~bool_splits) < min_samples_leaf):\n",
    "                    continue\n",
    "\n",
    "                # Calculate the info gain and update the best gain and question if necessary\n",
    "                gain = info_gain(data, data[bool_splits], data[~bool_splits], tree_type)\n",
    "                if gain > current_gain:\n",
    "                    current_gain, current_q = gain, (i, value, True)\n",
    "        \n",
    "        # If it is not numeric, get the unique values and the size\n",
    "        else:\n",
    "            categories = np.unique(screen)\n",
    "            cat_size = len(categories)\n",
    "\n",
    "            # If there are only two categories, there is only one possible split\n",
    "            if cat_size == 2:\n",
    "                cat_splits = [[0]]\n",
    "\n",
    "            # If there are more than two categories, there are many different possible splits. Split individually and then choose random splits\n",
    "            else:\n",
    "                cat_splits = [i for i in range(cat_size)]\n",
    "                random_splits = [np.random.choice(categories, size, replace=False) for size in np.arange(2, cat_size-1, 1)]\n",
    "                if len(random_splits) > 0:\n",
    "                    cat_splits.append(random_splits)\n",
    "                \n",
    "            # Get the unique splits\n",
    "            for split in cat_splits:\n",
    "                bool_splits = np.isin(screen, split)\n",
    "\n",
    "                # If the partition is too small, skip it\n",
    "                if (np.sum(bool_splits) < min_samples_leaf) or (np.sum(~bool_splits) < min_samples_leaf):\n",
    "                    continue\n",
    "            \n",
    "                # Calculate the info gain and update the best gain and question if necessary\n",
    "                gain = info_gain(data, data[bool_splits], data[~bool_splits], tree_type)\n",
    "                if gain > current_gain:\n",
    "                    current_gain, current_q = gain, (i, split, False)\n",
    "    \n",
    "    # If there is no best question, return None, otherwise return the best question\n",
    "    if current_q is None:\n",
    "        return 0, None\n",
    "    else:\n",
    "        return current_gain, Question(current_q[0], current_q[1], current_q[2])\n",
    "\n",
    "# Define a function that predicts a decision tree\n",
    "def predict_tree(sample, my_tree, predictions, joint_pdf):\n",
    "    \"\"\"Predict the label for a sample given a pre-made decision tree\n",
    "    Parameters:\n",
    "        sample (ndarray): a single sample\n",
    "        my_tree (Decision_Node or Leaf): a decision tree\n",
    "    Returns:\n",
    "        Label to be assigned to new sample\"\"\"\n",
    "\n",
    "    # Base case if my_tree is a leaf\n",
    "    if isinstance(my_tree, Leaf):\n",
    "        if joint_pdf:\n",
    "            # Gets the total weight of the leaf\n",
    "            predictions[:] = my_tree.get_train_size()\n",
    "        else:\n",
    "            # Gets the most common label of the leaf\n",
    "            predictions[:] = max(my_tree.prediction, key=my_tree.prediction.get)\n",
    "        return predictions\n",
    "\n",
    "    # Otherwise break it down into left and right branches\n",
    "    bools = my_tree.question.match(sample)\n",
    "    left, right = sample[bools], sample[~bools]\n",
    "\n",
    "    # Recursively call predict_tree on the left and right branches to fill in the predictions array\n",
    "    predictions[bools] = predict_tree(left, my_tree.left, predictions[bools], joint_pdf)\n",
    "    predictions[~bools] = predict_tree(right, my_tree.right, predictions[~bools], joint_pdf)\n",
    "    return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Classes (DecisionTree, AdaptiveForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \"\"\"\n",
    "    A decision tree class that can be used for classification.\n",
    "    Attributes:\n",
    "        data (numpy array): data to use for the tree\n",
    "        features (list): list of feature names\n",
    "        min_samples_leaf (int): minimum number of samples in a leaf\n",
    "        max_depth (int): maximum depth of the tree\n",
    "        is_numeric (numpy array - boolean): boolean array indicating whether each feature is numeric or not\n",
    "        size_random_subset (int): number of features to use for each split\n",
    "        y (numpy array): target variable\n",
    "        X (numpy array): data without the target variable\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, tree_type, features, min_samples_leaf, max_depth, is_numeric, size_random_subset):\n",
    "        \"\"\"\n",
    "        Initialize the decision tree\n",
    "        Inputs:\n",
    "            data (numpy array): data to use for the tree (target in last column)\n",
    "            tree_type (str): type of tree (classification or regression)\n",
    "            features (list): list of feature names\n",
    "            min_samples_leaf (int): minimum number of samples in a leaf\n",
    "            max_depth (int): maximum depth of the tree\n",
    "            is_numeric (numpy array - boolean): boolean array indicating whether each feature is numeric or not\n",
    "            size_random_subset (int): number of features to use for each split\n",
    "        \"\"\"\n",
    "        # Initialize the different attributes of the tree\n",
    "        self.data = data\n",
    "        self.tree_type = tree_type\n",
    "        self.features = features\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_depth = max_depth\n",
    "        self.is_numeric = is_numeric\n",
    "        self.size_random_subset = size_random_subset\n",
    "\n",
    "    # Define a method to build a tree\n",
    "    def build_tree(self, data, current_depth=0):\n",
    "        \"\"\"Build a classification tree using the classes Decision_Node and Leaf\n",
    "        Parameters:\n",
    "            data (ndarray)\n",
    "            min_samples_leaf (int): minimum allowed number of samples per leaf\n",
    "            max_depth (int): maximum allowed depth\n",
    "            current_depth (int): depth counter\n",
    "            random_subset (bool): whether or not to train on a random subset of features\n",
    "        Returns:\n",
    "            Decision_Node (or Leaf)\"\"\"\n",
    "        # If the number of rows is less than the minimum samples per leaf, return a leaf\n",
    "        if data.shape[0] < 2*self.min_samples_leaf:\n",
    "            return Leaf(data)\n",
    "        \n",
    "        # Find the best question to ask and return a leaf if there is no gain or past max depth\n",
    "        gain, question = find_best_split(data, self.tree_type, self.size_random_subset, self.min_samples_leaf, self.is_numeric)\n",
    "        if gain == 0 or current_depth >= self.max_depth:\n",
    "            return Leaf(data)\n",
    "        \n",
    "        # Partition the data and build the left and right branches\n",
    "        left, right = question.split(data)\n",
    "        left_branch = self.build_tree(left, current_depth+1)\n",
    "        right_branch = self.build_tree(right, current_depth+1)\n",
    "\n",
    "        # Return a Decision_Node with the best question and branches\n",
    "        return Decision_Node(question, left_branch, right_branch)\n",
    "    \n",
    "    # Define a method to fit the tree\n",
    "    def fit(self):\n",
    "        self.tree = self.build_tree(self.data)\n",
    "\n",
    "    # Define a method to predict the labels for a sample\n",
    "    def predict(self, sample, joint_pdf = False):\n",
    "        \"\"\"Predict the label for a sample\n",
    "        Parameters:\n",
    "            sample (ndarray): a single sample\n",
    "        Returns:\n",
    "            Label to be assigned to new sample\"\"\"\n",
    "        # Make sure the sample is a 2D array\n",
    "        if len(sample.shape) == 1:\n",
    "            sample = sample.reshape(1, -1)\n",
    "\n",
    "        # Initialize the predictions array and predict the label\n",
    "        predictions = np.zeros(len(sample))\n",
    "        return predict_tree(sample, self.tree, predictions, joint_pdf)\n",
    "        \n",
    "class AdaptiveForest:\n",
    "    \"\"\" A random forest class that can be used for classification or regression.\n",
    "    Attributes:\n",
    "        data (numpy array): data to use for the forest\n",
    "        features (list): list of feature names\n",
    "        min_samples_leaf (int): minimum number of samples in a leaf\n",
    "        max_depth (int): maximum depth of the tree\n",
    "        is_numeric (numpy array - boolean): boolean array indicating whether each feature is numeric or not\n",
    "        category_codes (dict): dictionary of the category codes for each non numeric feature\n",
    "        size_random_subset (int): number of features to use for each split\n",
    "        n_trees (int): number of trees in the forest\n",
    "        weights (numpy array): weights for each tree\n",
    "        trees (list): list of trees in the forest\n",
    "        bootstrap_size (int): size of the boot strap sample\n",
    "        data_size (int): size of the data\n",
    "        class_n (int): number of classes\n",
    "        forest_type (str): type of forest (classification or regression)\n",
    "\n",
    "    Methods:\n",
    "        access_tree: access the decision tree class\n",
    "        print_codes: print the category codes\n",
    "        fit: fit the random forest\n",
    "        fit_with_oob: fit the random forest with out of bag data\n",
    "        predict: predict the label for a sample with the ensemble\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,dataframe, labels = None, target_index = None, n_trees = 10, min_samples_leaf=5, max_depth=4, features = None, size_rand_subset = None, bootstrap_size = None):\n",
    "        \"\"\"\n",
    "        Initialize the random forest\n",
    "        Inputs:\n",
    "            dataframe (pandas dataframe or numpy array): data to use for the forest (target in last column)\n",
    "            labels (numpy array): labels to use for the forest\n",
    "            target_index (int): index of the target variable\n",
    "            n_trees (int): number of trees in the forest\n",
    "            min_samples_leaf (int): minimum number of samples in a leaf\n",
    "            max_depth (int): maximum depth of the tree\n",
    "            features (list): list of feature names\n",
    "            size_rand_subset (int): number of features to use for each split\n",
    "            bootstrap_size (int): size of the boot strap sample\n",
    "        \"\"\"\n",
    "        #### Data preprocessing ####\n",
    "        # Check the data type and raise an error if it is not a dataframe or numpy array\n",
    "        if isinstance(dataframe, np.ndarray):\n",
    "            df = pd.DataFrame(dataframe)\n",
    "\n",
    "            # Check if the data is numeric and convert it if it is not\n",
    "            def infer_dtype(col):\n",
    "                try:\n",
    "                    return pd.to_numeric(col, errors='raise')\n",
    "                except:\n",
    "                    return col\n",
    "            df = df.apply(lambda col: infer_dtype(col))\n",
    "            if features is not None:\n",
    "                df.columns = features\n",
    "        \n",
    "        # If the data is a dataframe, then make a copy of it, raise an error otherwise\n",
    "        elif isinstance(dataframe, pd.DataFrame):\n",
    "            df = dataframe.copy(deep=True)\n",
    "        else:\n",
    "            raise ValueError('Data must be a pandas dataframe or numpy array')\n",
    "\n",
    "        # If labels are given, then make them a series and concatenate them to the dataframe if conditions met\n",
    "        if labels is not None:\n",
    "            labels = pd.Series(labels)\n",
    "            if target_index is not None:\n",
    "                raise ValueError('Cannot have both labels and target index')\n",
    "            \n",
    "            # check if labels and dataframe have the same number of rows\n",
    "            if len(labels) != len(df):\n",
    "                raise ValueError('Labels and dataframe must be same size')\n",
    "            \n",
    "            # concatenate the labels to the dataframe\n",
    "            labels.name = 'target'\n",
    "            df = pd.concat([df, labels], axis=1)\n",
    "\n",
    "        # If labels are not given, then check if target index is given and move the target variable to the last column\n",
    "        else:\n",
    "            if target_index is None:\n",
    "                print('Warning: No target index given. Using last index as target variable.')\n",
    "            else:\n",
    "                cols = list(df.columns)\n",
    "                cols.append(cols.pop(target_index))\n",
    "                df = df[cols]\n",
    "\n",
    "        #### Initialize tree attributes ####\n",
    "        # Save the features except for the target variable and check the target column data type\n",
    "        self.features = np.array(df.columns)\n",
    "        target_dtype = df[self.features[-1]].dtype\n",
    "\n",
    "        # check if the target variable is numeric\n",
    "        if isinstance(target_dtype, (int, float, complex)):\n",
    "            self.forest_type = 'regression'\n",
    "        else:\n",
    "            self.forest_type = 'classification'\n",
    "\n",
    "        # Check which features are numeric and save the boolean mask\n",
    "        numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "        self.is_numeric = df.columns.isin(numeric_cols)\n",
    "        self.category_codes = {}\n",
    "     \n",
    "        # Loop through non numeric columns and convert them to numeric while saving the category codes\n",
    "        for col in self.features[~self.is_numeric]:\n",
    "            self.category_codes[col] = dict(enumerate(df[col].astype('category').cat.categories))\n",
    "            df[col] = df[col].astype('category').cat.codes\n",
    "    \n",
    "        # Save the size of the random subset for attribute bagging\n",
    "        if size_rand_subset is None:\n",
    "            self.size_rand_subset = int(np.ceil(np.sqrt(len(self.features) - 1)))\n",
    "        else:\n",
    "            self.size_rand_subset = size_rand_subset\n",
    "\n",
    "        # Initialize data and min_samples_leaf\n",
    "        self.data = df.values.astype(float)\n",
    "        if min_samples_leaf is None:\n",
    "            self.min_samples_leaf = 1\n",
    "        else:\n",
    "            self.min_samples_leaf = min_samples_leaf\n",
    "\n",
    "        # Initialize the max depth\n",
    "        if max_depth is None:\n",
    "            self.max_depth = np.inf\n",
    "        else:\n",
    "            self.max_depth = max_depth\n",
    "\n",
    "        # Save the data size and number of classes\n",
    "        self.data_size = self.data.shape[0]\n",
    "        self.class_n = len(np.unique(self.data[:,-1]))\n",
    "\n",
    "        #### Other forest attributes ####\n",
    "        # Save the boot strap size if given\n",
    "        if bootstrap_size is None:\n",
    "            self.bootstrap_size = df.shape[0]\n",
    "        else:\n",
    "            self.bootstrap_size = bootstrap_size\n",
    "\n",
    "        # Save the number of trees, the weights, and initialize the trees\n",
    "        self.n_trees = n_trees\n",
    "        self.weights = np.ones(self.n_trees) / self.n_trees\n",
    "        self.trees = []\n",
    "        self.data_weights = np.ones(self.data_size) / self.data_size\n",
    "\n",
    "    def access_tree(self):\n",
    "        \"\"\" Access the decision tree class \"\"\"\n",
    "        return DecisionTree(self.data, self.forest_type, self.features, self.min_samples_leaf, self.max_depth, self.is_numeric, self.size_rand_subset)\n",
    "        \n",
    "    def print_codes(self):\n",
    "        \"\"\" Print the category codes \"\"\"\n",
    "        max_key_length = max(len(str(key)) for key in self.category_codes)\n",
    "        for key in self.category_codes:\n",
    "            print(\"{:<{width}} {}\".format(str(key), \"---  \"+str(self.category_codes[key]), width=max_key_length + 1))\n",
    "\n",
    "    def fit(self, bootstrap_size = None):\n",
    "        \"\"\" Fit the random forest \"\"\"\n",
    "        self.trees = []\n",
    "        # If a new bootstrap size is given, update the bootstrap size\n",
    "        if bootstrap_size is not None:\n",
    "            self.bootstrap_size = bootstrap_size\n",
    "\n",
    "        # Loop through the number of trees and fit each tree to bootstrapped data\n",
    "        for _ in range(self.n_trees):\n",
    "            # boot strap our data, build a tree, and fit the tree\n",
    "            indices = np.random.choice(self.data_size, self.bootstrap_size, replace=True, p=self.data_weights)\n",
    "            sample = self.data[indices]\n",
    "            tree = DecisionTree(sample, self.forest_type, self.features, self.min_samples_leaf, self.max_depth, self.is_numeric, self.size_rand_subset)\n",
    "            tree.fit()\n",
    "\n",
    "            # append the tree to the forest\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def fit_with_oob(self):\n",
    "        raise NotImplementedError('Out of bag not yet implemented')\n",
    "        # get the out of bag data\n",
    "        # oob_indices = np.setdiff1d(np.arange(n_samples), indices, assume_unique=True)\n",
    "        # oob_data = self.data[oob_indices]\n",
    "        # self.oob_data.append(oob_data)\n",
    "    \n",
    "    def predict(self, sample, weighted = False, labeled = False):\n",
    "        \"\"\" Predict the label for a sample with the ensemble\"\"\"\n",
    "        # Initialize the ensemble predictions and loop through the trees\n",
    "        ensemble = np.zeros((len(sample),self.n_trees))\n",
    "        for i in range(self.n_trees):\n",
    "            ensemble[:,i] = self.trees[i].predict(sample)\n",
    "\n",
    "        # Determine the weights to use\n",
    "        weights = None\n",
    "        if weighted:\n",
    "            weights = self.weights\n",
    "\n",
    "        # Get the predicted labels\n",
    "        ensemble = ensemble.astype(int)\n",
    "        predicted = np.apply_along_axis(lambda x: np.argmax(np.bincount(x, minlength=np.max(ensemble)+1, weights=weights)), axis=1, arr=ensemble)\n",
    "\n",
    "        # If the labels are not given, return the predicted labels\n",
    "        if not labeled:\n",
    "            return predicted\n",
    "        \n",
    "        # Otherwise return a vector with the category codes\n",
    "        else:\n",
    "            return np.vectorize(self.category_codes[self.features[-1]].get)(predicted.astype(int))\n",
    "        \n",
    "    def cluster_probs(self, true_pop, subset, norm=2):\n",
    "        # Get the matrix of norms between the true population and the subset\n",
    "        diff = np.linalg.norm(true_pop[:,np.newaxis,:] - subset[np.newaxis,:,:], ord=norm, axis=2)\n",
    "        \n",
    "        # find the closest subset point for each true population point\n",
    "        closest = np.argmin(diff, axis=1)\n",
    "\n",
    "        # Count the number of close true population points for each subset point and return the normalized counts\n",
    "        counts = np.bincount(closest, minlength=subset.shape[0]) + 1\n",
    "        return counts / np.sum(counts)\n",
    "    \n",
    "    def update_weights(self, sample):\n",
    "        \"\"\" Update the weights of the trees\n",
    "        Parameters:\n",
    "            sample (ndarray): a sample of the data not including the label\n",
    "        \"\"\"\n",
    "        # Get the leaf probabilities and update the weights\n",
    "        self.data_weights = self.cluster_probs(sample, self.data[:,:-1])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.976\n",
      "Time to initialize: 0.007\n",
      "Time to predict: 0.100\n",
      "Accuracy: 0.973\n"
     ]
    }
   ],
   "source": [
    "# upload the small minst data set\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# load the data\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# fit random forest classifier with 100 trees\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# get predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# get accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy: {:.3f}'.format(accuracy))\n",
    "\n",
    "# get confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "#display(cm)\n",
    "\n",
    "import time\n",
    "# Do the same for adaptive forest\n",
    "start = time.time()\n",
    "af = AdaptiveForest(X_train, y_train, n_trees=100, min_samples_leaf=1, max_depth=None)\n",
    "end = time.time()\n",
    "end = time.time()\n",
    "print('Time to initialize: {:.3f}'.format(end-start))\n",
    "start = time.time()\n",
    "af.fit()\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "y_pred = af.predict(X_test, weighted=True)\n",
    "end = time.time()\n",
    "print('Time to predict: {:.3f}'.format(end-start))\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy: {:.3f}'.format(accuracy))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "#display(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_digits(digits,biased_digits,sizes, sample_size=300):\n",
    "    \"\"\"\n",
    "    Take a biased sample of certain digits according to the sizes given\n",
    "    Parameters:\n",
    "        digits (ndarray): array of digits to sample from\n",
    "        biased_digits (ndarray): array of digits to sample more or less of\n",
    "        sizes (ndarray): array of weights for each digit\n",
    "\n",
    "    Returns:\n",
    "        (ndarray): array of indices of digits sampled\n",
    "    \"\"\"\n",
    "    max_bias_limit = 1/3\n",
    "\n",
    "    # Check that the digits and sizes are the same length\n",
    "    if len(biased_digits) != len(sizes):\n",
    "        raise ValueError('Digits and sizes must be the same length')\n",
    "    \n",
    "    # Make sure that they are all arrays\n",
    "    digits = np.array(digits)\n",
    "    biased_digits = np.array(biased_digits)\n",
    "\n",
    "    # Get the counts of each digit and initialize the digit sizes\n",
    "    counts = np.bincount(digits, minlength=10).astype(float)\n",
    "    digit_sizes = np.ones(10)\n",
    "\n",
    "    # Get digit sizes for the biased digits\n",
    "    for i in range(10):\n",
    "        if i in biased_digits:\n",
    "            digit_sizes[i] = sizes[np.where(biased_digits == i)[0][0]]\n",
    "\n",
    "    # normalize units\n",
    "    digit_sizes = digit_sizes / np.sum(digit_sizes)\n",
    "    draw_size = np.round(sample_size * digit_sizes)\n",
    "    testing = draw_size / counts\n",
    "\n",
    "    # Check that the testing is within the max bias limit\n",
    "    if np.any(testing > max_bias_limit):\n",
    "        raise ValueError('Reduce sample size given the bias weights')\n",
    "    \n",
    "    def sample_from_counts(digits, draw_size):\n",
    "        \"\"\" Sample from the darw_size \"\"\"\n",
    "        ind_list = []\n",
    "\n",
    "        # Loop through the digits and get the indices of each digit\n",
    "        for i in range(10):\n",
    "            single_digit = (digits == i)\n",
    "            digit_indices = np.where(single_digit)[0]\n",
    "            indices = np.random.choice(digit_indices, int(draw_size[i]), replace=False)\n",
    "\n",
    "            # Append the indices to the list\n",
    "            ind_list.append(indices)\n",
    "\n",
    "        # Return the indices\n",
    "        return np.concatenate(ind_list)\n",
    "    \n",
    "    # Get the indices of the biased digits and the remaining digits\n",
    "    biased_indices = sample_from_counts(digits, draw_size)\n",
    "    remaining_indices = np.setdiff1d(np.arange(len(digits)), biased_indices)\n",
    "    remaining_digits = digits[remaining_indices]\n",
    "\n",
    "    # Get the unbiased draw size\n",
    "    unbiased_ratio = counts / np.sum(counts)\n",
    "    min_dist = np.min(counts - draw_size)\n",
    "    min_dist_ind = np.argmin(counts - draw_size)\n",
    "    scale = min_dist / unbiased_ratio[min_dist_ind]\n",
    "    unbiased_draw_size = np.round(scale * unbiased_ratio) - 2\n",
    "    \n",
    "    # Get the indices of the unbiased digits and return the biased and unbiased indices\n",
    "    unbiased_indices = sample_from_counts(remaining_digits, unbiased_draw_size)\n",
    "    return biased_indices, unbiased_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy: 0.895\n",
      "AF Accuracy: 0.865\n",
      "AF Accuracy with weights: 0.913\n"
     ]
    }
   ],
   "source": [
    "bias,unbias = bias_digits(y,np.array([6,5,7,1]),np.array([.5,2,1.7,.2]), sample_size=300)\n",
    "x_train,x_test,y_train,y_test = X[bias],X[unbias],y[bias],y[unbias]\n",
    "\n",
    "# fit random forest classifier with 100 trees\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "# get predictions\n",
    "y_pred = rf.predict(x_test)\n",
    "\n",
    "# get accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('RF Accuracy: {:.3f}'.format(accuracy))\n",
    "\n",
    "# repeat for adaptive forest\n",
    "af = AdaptiveForest(x_train, y_train, n_trees=100, min_samples_leaf=1, max_depth=None)\n",
    "af.fit()\n",
    "y_pred = af.predict(x_test, weighted=True)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('AF Accuracy: {:.3f}'.format(accuracy))\n",
    "\n",
    "# update the weights\n",
    "af.update_weights(x_test)\n",
    "af.fit()\n",
    "y_pred = af.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('AF Accuracy with weights: {:.3f}'.format(accuracy))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "updater = X\n",
    "af.update_weights(updater)\n",
    "plt.hist(af.data_weights)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
