{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change the ranomd seed or n\n",
    "np.random.seed(0)\n",
    "n = 250\n",
    "smart_factor = np.random.normal(0, 15, n)\n",
    "smart_factor_norm = smart_factor / np.max(smart_factor)\n",
    "iq = 100 + smart_factor\n",
    "iq = iq.astype(int)\n",
    "education = np.round(14 + np.random.randint(-4, 2, n) + smart_factor_norm * 5).astype(int)\n",
    "female = np.random.randint(0, 2, n).astype(bool)\n",
    "age = 30 + np.random.randint(-10, 20, n)\n",
    "income_level = np.random.choice(['low', 'medium', 'high'], n, p=[0.4, 0.4, 0.2])\n",
    "criminal_val = -.15 *iq + -2.5*education + -5*female + -15*(income_level == 'high') + -10*(income_level == 'medium') + -10 * (age > 28) + np.random.randint(-20, 20, n)\n",
    "\n",
    "# take the bottom 10% of the criminal values and label them as criminals\n",
    "criminals = criminal_val > np.percentile(criminal_val, 90)\n",
    "\n",
    "# construct a pandas dataframe with the data\n",
    "df = pd.DataFrame({'iq': iq, 'education': education, 'female': female, 'age': age, 'income_level': income_level, 'criminals': criminals})\n",
    "df.loc[3, 'criminals'] = True\n",
    "df.loc[246, 'criminals'] = True\n",
    "crime_data = df.__deepcopy__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a function that returns the number of rows in a given array\n",
    "def num_rows(array):\n",
    "    \"\"\" Returns the number of rows in a given array \"\"\"\n",
    "    if array is None:\n",
    "        return 0\n",
    "    elif len(array.shape) == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return array.shape[0]\n",
    "\n",
    "# Define a function that gives number of samples under each class label\n",
    "def class_counts(data):\n",
    "    \"\"\" Returns a dictionary with the number of samples under each class label\n",
    "        formatted {label : number_of_samples} \"\"\"\n",
    "    if len(data.shape) == 1: # If there's only one row\n",
    "        return {data[-1] : 1}\n",
    "    counts = {}\n",
    "    for label in data[:,-1]:\n",
    "        if label not in counts:\n",
    "            counts[label] = 0\n",
    "        counts[label] += 1\n",
    "    return counts\n",
    "\n",
    "# Define a function that calculates the Gini impurity of a given array\n",
    "def info_gain(data, left, right):\n",
    "    \"\"\"Return the info gain of a partition of data.\n",
    "    Parameters:\n",
    "        data (ndarray): the unsplit data\n",
    "        left (ndarray): left split of data\n",
    "        right (ndarray): right split of data\n",
    "    Returns:\n",
    "        (float): info gain of the data\"\"\"\n",
    "        \n",
    "    def gini(data):\n",
    "        \"\"\"Return the Gini impurity of given array of data.\n",
    "        Parameters:\n",
    "            data (ndarray): data to examine\n",
    "        Returns:\n",
    "            (float): Gini impurity of the data\"\"\"\n",
    "        counts = class_counts(data)\n",
    "        N = num_rows(data)\n",
    "        impurity = 1\n",
    "        for lbl in counts:\n",
    "            prob_of_lbl = counts[lbl] / N\n",
    "            impurity -= prob_of_lbl**2\n",
    "        return impurity\n",
    "        \n",
    "    p = num_rows(right)/(num_rows(left)+num_rows(right))\n",
    "    return gini(data) - p*gini(right)-(1-p)*gini(left)\n",
    "\n",
    "\n",
    "\n",
    "############## WORK HERE ##############\n",
    "def find_best_split(data, tree_type, size_rand_subset, min_samples_leaf, is_numeric):\n",
    "    # Get feature_choice, define refinement, and initialize the best gain and current_bool\n",
    "    feature_choice = np.random.choice(data.shape[1], size_rand_subset, replace=False)\n",
    "    refinement = 100/np.min([1+np.abs(data.shape[0] -2*min_samples_leaf -2)// min_samples_leaf,40])\n",
    "    current_bool = None\n",
    "    current_gain = 0\n",
    "\n",
    "    # Initialize the best gain and question and loop through the first n-1 columns\n",
    "    for i in feature_choice:\n",
    "        screen = data[:,i]\n",
    "\n",
    "        # if it is numeric, get the unique split values based on the refinement\n",
    "        if is_numeric[i]:\n",
    "            split_vals = np.unique(np.percentile(screen, np.arange(refinement, 100, refinement)))\n",
    "\n",
    "            # Loop through the split values and partition the data\n",
    "            for value in split_vals:\n",
    "                bool_splits = (screen >= value) \n",
    "\n",
    "                # If the partition is too small, skip it\n",
    "                if (np.sum(bool_splits) < min_samples_leaf) or (np.sum(~bool_splits) < min_samples_leaf):\n",
    "                    continue\n",
    "\n",
    "                # Calculate the info gain and update the best gain and question if necessary\n",
    "                gain = info_gain(data, data[bool_splits], data[~bool_splits])\n",
    "                if gain > current_gain:\n",
    "                    current_gain, current_bool = gain, bool_splits\n",
    "        \n",
    "        # If it is not numeric, get the unique values and the size\n",
    "        else:\n",
    "            categories = np.unique(screen)\n",
    "            cat_size = len(categories)\n",
    "\n",
    "            # If there are only two categories, there is only one possible split\n",
    "            if cat_size == 2:\n",
    "                cat_splits = [[0]]\n",
    "\n",
    "            # If there are more than two categories, there are many different possible splits. Split individually and then choose random splits\n",
    "            else:\n",
    "                cat_splits = [i for i in range(cat_size)]\n",
    "                random_splits = [np.random.choice(categories, size, replace=False) for size in np.arange(2, cat_size-1, 1)]\n",
    "                if len(random_splits) > 0:\n",
    "                    cat_splits.append(random_splits)\n",
    "                \n",
    "            # Get the unique splits\n",
    "            for split in cat_splits:\n",
    "                bool_splits = np.isin(screen, split)\n",
    "\n",
    "                # If the partition is too small, skip it\n",
    "                if (np.sum(bool_splits) < min_samples_leaf) or (np.sum(~bool_splits) < min_samples_leaf):\n",
    "                    continue\n",
    "            \n",
    "                # Calculate the info gain and update the best gain and question if necessary\n",
    "                gain = info_gain(data, data[bool_splits], data[~bool_splits])\n",
    "                if gain > current_gain:\n",
    "                    current_gain, current_bool = gain, bool_splits\n",
    "\n",
    "    # Return the best gain and split\n",
    "    return current_gain, current_bool\n",
    "\n",
    "\n",
    "# Wilson's Code\n",
    "def find_best_split(data, tree_type, size_rand_subset, min_samples_leaf, is_numeric):\n",
    "    \"\"\"Find the optimal split\n",
    "    Parameters:\n",
    "        data (ndarray): Data in question\n",
    "        feature_names (list of strings): Labels for each column of data\n",
    "        min_samples_leaf (int): minimum number of samples per leaf\n",
    "        random_subset (bool): for Problem 6\n",
    "    Returns:\n",
    "        (float): Best info gain\n",
    "        (Question): Best question\"\"\"\n",
    "    # Initialize variables\n",
    "    best_gain = 0\n",
    "    best_question = None\n",
    "\n",
    "    m, n = data.shape\n",
    "    n -= 1\n",
    "    feature_indices = np.arange(n)\n",
    "\n",
    "    if random_subset:  # If it is random\n",
    "        num_feat = int(np.sqrt(n))\n",
    "        feature_indices = np.random.choice(feature_indices, num_feat, replace=False)\n",
    "\n",
    "    # Iterate through each feature (column) (Do not iterate through final column\n",
    "    for j in feature_indices:\n",
    "        # Iterate through each unique value (row)\n",
    "        for i in range(m):\n",
    "            # Create Question obj with column and value\n",
    "            question = Question(j, data[i, j], feature_names=feature_names)\n",
    "\n",
    "            # Use partition to split the dataset into left and right partitions\n",
    "            left, right = partition(data, question)\n",
    "\n",
    "            # If either left or right partitions are smaller than allowable leaf size reject\n",
    "            if num_rows(left) < min_samples_leaf:\n",
    "                continue\n",
    "            elif num_rows(right) < min_samples_leaf:\n",
    "                continue\n",
    "\n",
    "            # Calculate Info Gain\n",
    "            gain = info_gain(data, left, right)\n",
    "\n",
    "            # Update best_gain and best_question\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_question = question\n",
    "\n",
    "    return best_gain, best_question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### REFERENCE ####\n",
    "def predict_tree(sample, my_tree):\n",
    "    \"\"\"Predict the label for a sample given a pre-made decision tree\n",
    "    Parameters:\n",
    "        sample (ndarray): a single sample\n",
    "        my_tree (Decision_Node or Leaf): a decision tree\n",
    "    Returns:\n",
    "        Label to be assigned to new sample\"\"\"\n",
    "\n",
    "    # Base case if my_tree is a leaf\n",
    "    if isinstance(my_tree, Leaf):\n",
    "        return max(my_tree.prediction, key=my_tree.prediction.get)\n",
    "\n",
    "    # Otherwise break it down into left and right branches\n",
    "    if my_tree.question.match(sample):\n",
    "        return predict_tree(sample, my_tree.left)\n",
    "    else:\n",
    "        return predict_tree(sample, my_tree.right)\n",
    "\n",
    "\n",
    "def analyze_tree(dataset, my_tree):\n",
    "    \"\"\"Test how accurately a tree classifies a dataset\n",
    "    Parameters:\n",
    "        dataset (ndarray): Labeled data with the labels in the last column\n",
    "        tree (Decision_Node or Leaf): a decision tree\n",
    "    Returns:\n",
    "        (float): Proportion of dataset classified correctly\"\"\"\n",
    "\n",
    "    # Get labels\n",
    "    actual_labels = dataset[:, -1]\n",
    "    n = dataset.shape[0]\n",
    "\n",
    "    # Predict each sample in dataset\n",
    "    predicted_labels = np.array(\n",
    "        [predict_tree(dataset[i], my_tree) for i in range(n)])\n",
    "\n",
    "    return (actual_labels == predicted_labels).sum() / n\n",
    "\n",
    "\n",
    "def predict_forest(sample, forest):\n",
    "    \"\"\"Predict the label for a new sample, given a random forest\n",
    "    Parameters:\n",
    "        sample (ndarray): a single sample\n",
    "        forest (list): a list of decision trees\n",
    "    Returns:\n",
    "        Label to be assigned to new sample\"\"\"\n",
    "    n = len(forest)\n",
    "\n",
    "    # Predict the sample label using each tree\n",
    "    predicted_labels = np.array(\n",
    "        [predict_tree(sample, forest[i]) for i in range(n)]).astype(int)\n",
    "\n",
    "    return np.argmax(np.bincount(predicted_labels))\n",
    "\n",
    "\n",
    "def analyze_forest(dataset, forest):\n",
    "    \"\"\"Test how accurately a forest classifies a dataset\n",
    "    Parameters:\n",
    "        dataset (ndarray): Labeled data with the labels in the last column\n",
    "        forest (list): list of decision trees\n",
    "    Returns:\n",
    "        (float): Proportion of dataset classified correctly\"\"\"\n",
    "\n",
    "    # Get labels\n",
    "    actual_labels = dataset[:, -1]\n",
    "\n",
    "    n = dataset.shape[0]\n",
    "\n",
    "    # Predict each sample in the dataset\n",
    "    predicted_labels = np.array(\n",
    "        [predict_forest(dataset[i], forest) for i in range(n)])\n",
    "\n",
    "    return (actual_labels == predicted_labels).sum() / n\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Question:\n",
    "    \"\"\"Questions to use in construction and display of Decision Trees.\n",
    "    Attributes:\n",
    "        column (int): which column of the data this question asks\n",
    "        value (int/float): value the question asks about\n",
    "        features (str): name of the feature asked about\n",
    "    Methods:\n",
    "        match: returns boolean of if a given sample answered T/F\"\"\"\n",
    "\n",
    "    def __init__(self, column, value, feature_names):\n",
    "        # Store attributes\n",
    "        self.column = column\n",
    "        self.value = value\n",
    "        self.features = feature_names[self.column]\n",
    "\n",
    "    def match(self, sample):\n",
    "        \"\"\"Returns T/F depending on how the sample answers the question\n",
    "        Parameters:\n",
    "            sample ((n,), ndarray): New sample to classify\n",
    "        Returns:\n",
    "            (bool): How the sample compares to the question\"\"\"\n",
    "        # Check if it is a match\n",
    "        return sample[self.column] >= self.value\n",
    "\n",
    "\n",
    "class Leaf:\n",
    "    \"\"\"Tree leaf node\n",
    "    Attribute:\n",
    "        prediction (dict): Dictionary of labels at the leaf\"\"\"\n",
    "    def __init__(self, data):\n",
    "        # Store attributes\n",
    "        self.prediction = class_counts(data)\n",
    "        self.train_size = num_rows(data)\n",
    "    \n",
    "    # Define a method to get the train size\n",
    "    def get_train_size(self):\n",
    "        return self.train_size\n",
    "\n",
    "    # Define a method to update the test size\n",
    "    def update_test_size(self, data):\n",
    "        self.test_size = num_rows(data)\n",
    "\n",
    "    # Define a method to get the test size\n",
    "    def get_test_size(self):\n",
    "        return self.test_size\n",
    "\n",
    "\n",
    "class Decision_Node:\n",
    "    \"\"\"Tree node with a question\n",
    "    Attributes:\n",
    "        question (Question): Question associated with node\n",
    "        left (Decision_Node or Leaf): child branch\n",
    "        right (Decision_Node or Leaf): child branch\"\"\"\n",
    "    def __init__(self, question, left_branch, right_branch):\n",
    "        # Store attributes\n",
    "        self.question = question\n",
    "        self.left = left_branch\n",
    "        self.right = right_branch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Classes (DecisionTree, AdaptiveForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \"\"\"\n",
    "    A decision tree class that can be used for classification.\n",
    "    Attributes:\n",
    "        data (numpy array): data to use for the tree\n",
    "        features (list): list of feature names\n",
    "        min_samples_leaf (int): minimum number of samples in a leaf\n",
    "        max_depth (int): maximum depth of the tree\n",
    "        is_numeric (numpy array - boolean): boolean array indicating whether each feature is numeric or not\n",
    "        size_random_subset (int): number of features to use for each split\n",
    "        y (numpy array): target variable\n",
    "        X (numpy array): data without the target variable\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, tree_type, features, min_samples_leaf, max_depth, is_numeric, size_random_subset):\n",
    "        \"\"\"\n",
    "        Initialize the decision tree\n",
    "        Inputs:\n",
    "            data (numpy array): data to use for the tree (target in last column)\n",
    "            tree_type (str): type of tree (classification or regression)\n",
    "            features (list): list of feature names\n",
    "            min_samples_leaf (int): minimum number of samples in a leaf\n",
    "            max_depth (int): maximum depth of the tree\n",
    "            is_numeric (numpy array - boolean): boolean array indicating whether each feature is numeric or not\n",
    "            size_random_subset (int): number of features to use for each split\n",
    "        \"\"\"\n",
    "        # Initialize the different attributes of the tree\n",
    "        self.data = data\n",
    "        self.tree_type = tree_type\n",
    "        self.features = features\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_depth = max_depth\n",
    "        self.is_numeric = is_numeric\n",
    "        self.size_random_subset = size_random_subset\n",
    "\n",
    "    # Define a method to build a tree\n",
    "    def build_tree(self, data, current_depth=0):\n",
    "        \"\"\"Build a classification tree using the classes Decision_Node and Leaf\n",
    "        Parameters:\n",
    "            data (ndarray)\n",
    "            min_samples_leaf (int): minimum allowed number of samples per leaf\n",
    "            max_depth (int): maximum allowed depth\n",
    "            current_depth (int): depth counter\n",
    "            random_subset (bool): whether or not to train on a random subset of features\n",
    "        Returns:\n",
    "            Decision_Node (or Leaf)\"\"\"\n",
    "        # If the number of rows is less than the minimum samples per leaf, return a leaf\n",
    "        if data.shape[0] < 2*self.min_samples_leaf:\n",
    "            return Leaf(data)\n",
    "        \n",
    "        # Find the best question to ask and return a leaf if there is no gain or past max depth\n",
    "        gain, question = find_best_split(data, self.tree_type, self.size_random_subset, self.min_samples_leaf, self.is_numeric)\n",
    "        if gain == 0 or current_depth >= self.max_depth:\n",
    "            return Leaf(data)\n",
    "        \n",
    "        # Partition the data and build the left and right branches\n",
    "        bool_vals = question.match(data)\n",
    "        left, right = data[bool_vals], data[~bool_vals]\n",
    "        left_branch = self.build_tree(left, current_depth+1)\n",
    "        right_branch = self.build_tree(right, current_depth+1)\n",
    "\n",
    "        # Return a Decision_Node with the best question and branches\n",
    "        return Decision_Node(question, left_branch, right_branch)\n",
    "    \n",
    "    # Define a method to fit the tree\n",
    "    def fit(self):\n",
    "        self.tree = self.build_tree(self.data)\n",
    "\n",
    "\n",
    "class AdaptiveForest:\n",
    "    \"\"\"\n",
    "    A random forest class that can be used for classification.\n",
    "    Attributes:\n",
    "        data (numpy array): data to use for the tree\n",
    "        features (list): list of feature names\n",
    "        min_samples_leaf (int): minimum number of samples in a leaf\n",
    "        max_depth (int): maximum depth of the tree\n",
    "        is_numeric (numpy array - boolean): boolean array indicating whether each feature is numeric or not\n",
    "        size_random_subset (int): number of features to use for each split\n",
    "        y_index (int): index of the column of the target variable\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,dataframe, labels = None, target_index = None, n_trees = 10, min_samples_leaf=5, max_depth=4, features = None, size_rand_subset = None, bootstrap_size = None):\n",
    "        \"\"\"\n",
    "        Initialize the random forest\n",
    "        Inputs:\n",
    "            dataframe (pandas dataframe or numpy array): data to use for the forest\n",
    "            target_index (int): index of the column of the target variable\n",
    "            min_samples_leaf (int): minimum number of samples in a leaf\n",
    "            max_depth (int): maximum depth of the tree\n",
    "            features (list): list of feature names\n",
    "            is_numeric (numpy array - boolean): boolean array indicating whether each feature is numeric or not\n",
    "            category_codes (dict): dictionary of the category codes for each non numeric feature\n",
    "            size_random_subset (int): number of features to use for each split\n",
    "        \"\"\"\n",
    "        #### Data preprocessing ####\n",
    "        # Check the data type and raise an error if it is not a dataframe or numpy array\n",
    "        if isinstance(dataframe, np.ndarray):\n",
    "            df = pd.DataFrame(dataframe)\n",
    "\n",
    "            # Check if the data is numeric and convert it if it is not\n",
    "            def infer_dtype(col):\n",
    "                try:\n",
    "                    return pd.to_numeric(col, errors='raise')\n",
    "                except:\n",
    "                    return col\n",
    "            df = df.apply(lambda col: infer_dtype(col))\n",
    "            if features is not None:\n",
    "                df.columns = features\n",
    "        \n",
    "        # If the data is a dataframe, then make a copy of it, raise an error otherwise\n",
    "        elif isinstance(dataframe, pd.DataFrame):\n",
    "            df = dataframe.__deepcopy__()\n",
    "        else:\n",
    "            raise ValueError('Data must be a pandas dataframe or numpy array')\n",
    "\n",
    "        # If labels are given, then make them a series and concatenate them to the dataframe if conditions met\n",
    "        if labels is not None:\n",
    "            labels = pd.Series(labels)\n",
    "            if target_index is not None:\n",
    "                raise ValueError('Cannot have both labels and target index')\n",
    "            \n",
    "            # check if labels and dataframe have the same number of rows\n",
    "            if len(labels) != len(df):\n",
    "                raise ValueError('Labels and dataframe must be same size')\n",
    "            \n",
    "            # concatenate the labels to the dataframe\n",
    "            df = pd.concat([df, labels], axis=1)\n",
    "\n",
    "        # If labels are not given, then check if target index is given and move the target variable to the last column\n",
    "        else:\n",
    "            if target_index is None:\n",
    "                print('Warning: No target index given. Using last index as target variable.')\n",
    "            else:\n",
    "                cols = list(df.columns)\n",
    "                cols.append(cols.pop(target_index))\n",
    "                df = df[cols]\n",
    "\n",
    "        #### Initialize tree attributes ####\n",
    "        # Save the features except for the target variable and check the target column data type\n",
    "        self.features = np.array(df.columns)\n",
    "        target_dtype = df[self.features[-1]].dtype\n",
    "\n",
    "        # check if the target variable is numeric\n",
    "        if isinstance(target_dtype, (int, float, complex)):\n",
    "            self.forest_type = 'regression'\n",
    "        else:\n",
    "            self.forest_type = 'classification'\n",
    "\n",
    "        ######## TO DO: add regression functionality ########\n",
    "        if self.forest_type == 'regression':\n",
    "            raise ValueError('Regression not yet implemented')\n",
    "        \n",
    "        #####################################################\n",
    "\n",
    "        # Check which features are numeric and save the boolean mask\n",
    "        numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "        self.is_numeric = df.columns.isin(numeric_cols)\n",
    "        self.category_codes = {}\n",
    "     \n",
    "        # Loop through non numeric columns and convert them to numeric while saving the category codes\n",
    "        for col in self.features[~self.is_numeric]:\n",
    "            self.category_codes[col] = dict(enumerate(df[col].astype('category').cat.categories))\n",
    "            df[col] = df[col].astype('category').cat.codes\n",
    "    \n",
    "        # Save the size of the random subset for attribute bagging\n",
    "        if size_rand_subset is None:\n",
    "            self.size_rand_subset = int(np.ceil(np.sqrt(len(self.features) - 1)))\n",
    "        else:\n",
    "            self.size_rand_subset = size_rand_subset\n",
    "\n",
    "        # Initialize remaining tree attributes\n",
    "        self.data = df.values.astype(float)\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "        #### Other forest attributes ####\n",
    "        if bootstrap_size is None:\n",
    "            self.bootstrap_size = df.shape[0]\n",
    "        else:\n",
    "            self.bootstrap_size = bootstrap_size\n",
    "        self.n_trees = n_trees\n",
    "        self.weights = np.ones(self.n_trees) / self.n_trees\n",
    "\n",
    "\n",
    "    def access_tree(self):\n",
    "        \"\"\" Access the decision tree class \"\"\"\n",
    "        return DecisionTree(self.data, self.forest_type, self.features, self.min_samples_leaf, self.max_depth, self.is_numeric, self.size_rand_subset)\n",
    "        \n",
    "\n",
    "    ######## FIX ME ########\n",
    "    def fit(self, test_size = .33):\n",
    "        \"\"\" Fit the random forest \"\"\"\n",
    "        # Initialize the list of trees\n",
    "        self.trees = []\n",
    "        #self.oob_data = []\n",
    "\n",
    "        # Loop through the number of trees and fit each tree to bootstrapped data\n",
    "        for i in range(self.n_trees):\n",
    "            # boot strap our data\n",
    "            n_samples = len(self.data)\n",
    "            indices = np.random.randint(0, n_samples, size=n_samples)\n",
    "            boot_data = self.data[indices]\n",
    "\n",
    "            # get the out of bag data\n",
    "            # oob_indices = np.setdiff1d(np.arange(n_samples), indices, assume_unique=True)\n",
    "            # oob_data = self.data[oob_indices]\n",
    "            # self.oob_data.append(oob_data)\n",
    "\n",
    "            # Build the tree and fit it\n",
    "            tree = DecisionTree(boot_data, self.features, self.min_samples_leaf, self.max_depth, self.is_numeric, self.size_rand_subset)\n",
    "            tree.fit()\n",
    "\n",
    "            # Save the tree and the normalized weights\n",
    "            self.trees.append(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No target index given. Using last index as target variable.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'random_subset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/samlayton/Dropbox/Sam 2022/Work/Github/Personal Repositories/ML_and_Optimization/ML_and_Optimization/Random OLS Tree/model_doping.ipynb Cell 11\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/samlayton/Dropbox/Sam%202022/Work/Github/Personal%20Repositories/ML_and_Optimization/ML_and_Optimization/Random%20OLS%20Tree/model_doping.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m tester \u001b[39m=\u001b[39m AdaptiveForest(crime_data1)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/samlayton/Dropbox/Sam%202022/Work/Github/Personal%20Repositories/ML_and_Optimization/ML_and_Optimization/Random%20OLS%20Tree/model_doping.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m tree \u001b[39m=\u001b[39m tester\u001b[39m.\u001b[39maccess_tree()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/samlayton/Dropbox/Sam%202022/Work/Github/Personal%20Repositories/ML_and_Optimization/ML_and_Optimization/Random%20OLS%20Tree/model_doping.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m tree\u001b[39m.\u001b[39;49mfit()\n",
      "\u001b[1;32m/Users/samlayton/Dropbox/Sam 2022/Work/Github/Personal Repositories/ML_and_Optimization/ML_and_Optimization/Random OLS Tree/model_doping.ipynb Cell 11\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samlayton/Dropbox/Sam%202022/Work/Github/Personal%20Repositories/ML_and_Optimization/ML_and_Optimization/Random%20OLS%20Tree/model_doping.ipynb#W6sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/samlayton/Dropbox/Sam%202022/Work/Github/Personal%20Repositories/ML_and_Optimization/ML_and_Optimization/Random%20OLS%20Tree/model_doping.ipynb#W6sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtree \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild_tree(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata)\n",
      "\u001b[1;32m/Users/samlayton/Dropbox/Sam 2022/Work/Github/Personal Repositories/ML_and_Optimization/ML_and_Optimization/Random OLS Tree/model_doping.ipynb Cell 11\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samlayton/Dropbox/Sam%202022/Work/Github/Personal%20Repositories/ML_and_Optimization/ML_and_Optimization/Random%20OLS%20Tree/model_doping.ipynb#W6sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m Leaf(data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samlayton/Dropbox/Sam%202022/Work/Github/Personal%20Repositories/ML_and_Optimization/ML_and_Optimization/Random%20OLS%20Tree/model_doping.ipynb#W6sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m# Find the best question to ask and return a leaf if there is no gain or past max depth\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/samlayton/Dropbox/Sam%202022/Work/Github/Personal%20Repositories/ML_and_Optimization/ML_and_Optimization/Random%20OLS%20Tree/model_doping.ipynb#W6sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m gain, question \u001b[39m=\u001b[39m find_best_split(data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize_random_subset, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmin_samples_leaf, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mis_numeric)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samlayton/Dropbox/Sam%202022/Work/Github/Personal%20Repositories/ML_and_Optimization/ML_and_Optimization/Random%20OLS%20Tree/model_doping.ipynb#W6sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mif\u001b[39;00m gain \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m current_depth \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_depth:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samlayton/Dropbox/Sam%202022/Work/Github/Personal%20Repositories/ML_and_Optimization/ML_and_Optimization/Random%20OLS%20Tree/model_doping.ipynb#W6sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m Leaf(data)\n",
      "\u001b[1;32m/Users/samlayton/Dropbox/Sam 2022/Work/Github/Personal Repositories/ML_and_Optimization/ML_and_Optimization/Random OLS Tree/model_doping.ipynb Cell 11\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samlayton/Dropbox/Sam%202022/Work/Github/Personal%20Repositories/ML_and_Optimization/ML_and_Optimization/Random%20OLS%20Tree/model_doping.ipynb#W6sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m n \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samlayton/Dropbox/Sam%202022/Work/Github/Personal%20Repositories/ML_and_Optimization/ML_and_Optimization/Random%20OLS%20Tree/model_doping.ipynb#W6sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m feature_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(n)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/samlayton/Dropbox/Sam%202022/Work/Github/Personal%20Repositories/ML_and_Optimization/ML_and_Optimization/Random%20OLS%20Tree/model_doping.ipynb#W6sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39mif\u001b[39;00m random_subset:  \u001b[39m# If it is random\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samlayton/Dropbox/Sam%202022/Work/Github/Personal%20Repositories/ML_and_Optimization/ML_and_Optimization/Random%20OLS%20Tree/model_doping.ipynb#W6sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     num_feat \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39msqrt(n))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samlayton/Dropbox/Sam%202022/Work/Github/Personal%20Repositories/ML_and_Optimization/ML_and_Optimization/Random%20OLS%20Tree/model_doping.ipynb#W6sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     feature_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(feature_indices, num_feat, replace\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random_subset' is not defined"
     ]
    }
   ],
   "source": [
    "crime_data1 = crime_data.copy()\n",
    "crime_data1[crime_data1.columns[0]] += .1\n",
    "tester = AdaptiveForest(crime_data1)\n",
    "tree = tester.access_tree()\n",
    "\n",
    "tree.fit()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
